"""
----------Generated by the template----------

Examples of connection with MinIO, simulating a local Datalake.

"""

# Connection with MinIO
def _get_conn_from_env():
    """
    Generating an s3 client
    with the environment variables.
    """
    import os
    import boto3
    from botocore.client import Config
    from dotenv import load_dotenv

    load_dotenv()
    
    # Create a S3 client for MinIO
    try:
        s3 = boto3.client(
            's3',
            endpoint_url='http://minio:9000',
            aws_access_key_id=os.getenv('MINIO_ROOT_USER'),    # Access Key
            aws_secret_access_key=os.getenv('MINIO_ROOT_PASSWORD'), # Secret Key
            config=Config(signature_version='s3v4'),  # Used for MinIO
            region_name=os.getenv('MINIO_REGION'),
        )
    except Exception as e:
        print(f"Error establishing connection: {str(e)}")
        raise

    return s3


def list_minio_buckets(**kwargs):
    """
    List buckets in MinIO
    """

    try:
        s3 = _get_conn_from_env()
        response = s3.list_buckets()
        buckets = response.get('Buckets', [])
        
        if buckets:
            print("Buckets found on MinIO:")
            for bucket in buckets:
                print(f"- {bucket['Name']}")
            return buckets
        else:
            print("No buckets found in MinIO")
            return []
            
    except Exception as e:
        print(f"Error listing buckets: {str(e)}")
        raise


def list_bucket_content(**kwargs):
    """List the contents of an S3 bucket"""
    
    s3 = _get_conn_from_env()
    _bucket = 'bronze'

    response = s3.list_objects_v2(Bucket=_bucket)
    files = [file for file in response.get('Contents', [])]

    print(files)


def upload_bucket_content(**kwargs):
    """
    Upload file in bucket S3
    """
    import pandas as pd
    import numpy as np
    from io import StringIO

    s3 = _get_conn_from_env()
    
    # Mock data
    df = pd.DataFrame({
        'id': range(1, 101),
        'value1': np.random.choice(['A', 'B', 'C', 'D'], 100),
        'value2': np.random.randint(50, 500, 100)
    })
    
    print(df.head())
    print('#' * 100)
    
    csv_buffer = StringIO()
    df.to_csv(csv_buffer, index=False)

    # save on bucket
    s3.put_object(
        Bucket='bronze',
        Key='sample_data.csv',
        Body=csv_buffer.getvalue()
    )

def get_bucket_content(**kwargs):
    """
    Get file from S3 Bucket
    """
    import pandas as pd
    import numpy as np
    from io import StringIO

    s3 = _get_conn_from_env()
    response = s3.get_object(Bucket='bronze', Key='sample_data.csv')
    csv_content = response['Body'].read().decode('utf-8')
    df = pd.read_csv(StringIO(csv_content))
    print(f'Dataframe from BRONZE bucket: {df.shape}')
    print(df.head())
    print('#' * 100)

    # Create new rows and append data
    new_rows = pd.DataFrame({
        'id': range(100, 200),
        'value1': np.random.choice(['A', 'B', 'C', 'D'], 100),
        'value2': np.random.randint(50, 500, 100)
    })
    
    df = pd.concat([df, new_rows], ignore_index=True)
    df.columns = ['ID', 'VALUE_1', 'VALUE_2']
    
    print('New dataframe: ', df.shape)
    
    csv_buffer = StringIO()
    df.to_csv(csv_buffer, index=False)

    s3.put_object(
        Bucket='silver',
        Key='arquivo.csv',
        Body=csv_buffer.getvalue()
    )

